{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nimport collections \nimport os\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse.linalg import svds\nimport warnings; warnings.simplefilter('ignore')\n%matplotlib inline\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, concatenate, Flatten, Activation, Add, Dropout, Multiply\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_error\n\nfrom keras.optimizers import adam_v2\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:12:09.339011Z","iopub.execute_input":"2022-04-14T07:12:09.339333Z","iopub.status.idle":"2022-04-14T07:12:14.992151Z","shell.execute_reply.started":"2022-04-14T07:12:09.339238Z","shell.execute_reply":"2022-04-14T07:12:14.991336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data =pd.read_csv('/kaggle/input/amazon-product-ratings/ratings_Beauty.csv', names = [\"userId\", \"ProductId\", \"Ratings\", \"Timestamp\"])\ndata.head(15)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:12:14.994701Z","iopub.execute_input":"2022-04-14T07:12:14.995179Z","iopub.status.idle":"2022-04-14T07:12:17.659738Z","shell.execute_reply.started":"2022-04-14T07:12:14.99514Z","shell.execute_reply":"2022-04-14T07:12:17.65909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Check the distribution of the rating\nwith sns.axes_style('white'):\n    g = sns.factorplot(\"Ratings\", data=data, aspect=2.0,kind='count')\n    g.set_ylabels(\"Total number of ratings\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:12:17.660899Z","iopub.execute_input":"2022-04-14T07:12:17.661587Z","iopub.status.idle":"2022-04-14T07:12:18.885328Z","shell.execute_reply.started":"2022-04-14T07:12:17.66155Z","shell.execute_reply":"2022-04-14T07:12:18.884653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(['Timestamp'], axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:12:18.887139Z","iopub.execute_input":"2022-04-14T07:12:18.887802Z","iopub.status.idle":"2022-04-14T07:12:18.940207Z","shell.execute_reply.started":"2022-04-14T07:12:18.887762Z","shell.execute_reply":"2022-04-14T07:12:18.939495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sparsity of the dataset. The sparsity is found by dividing the total number of ratings to the product of unique users and products available in the dataset.\ntotal_unique_products = data[\"ProductId\"].nunique()\ntotal_unique_users =  data[\"userId\"].nunique()\ntotal_unique_ratings =  data[\"Ratings\"].count()\nprint(total_unique_products)\nprint(total_unique_ratings)\nprint(total_unique_users)\nsparsity = 1 - (total_unique_ratings/(total_unique_users * total_unique_products))\nprint(sparsity)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:12:18.941583Z","iopub.execute_input":"2022-04-14T07:12:18.941827Z","iopub.status.idle":"2022-04-14T07:12:19.74435Z","shell.execute_reply.started":"2022-04-14T07:12:18.941795Z","shell.execute_reply":"2022-04-14T07:12:19.743605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rated_products = data.groupby(by='userId',as_index=False)['Ratings'].count()\nprint(rated_products)\nrated_products = rated_products[rated_products['Ratings'] < 20]\nnew_dataset = data.loc[~((data.userId.isin(rated_products['userId']))),:]\nno_of_rated_products_per_user = new_dataset.groupby(by='userId')['Ratings'].count().sort_values(ascending=False)\nprint(no_of_rated_products_per_user)\nprint(new_dataset.ProductId.nunique())\nprint(new_dataset.userId.nunique())","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:12:19.745544Z","iopub.execute_input":"2022-04-14T07:12:19.746209Z","iopub.status.idle":"2022-04-14T07:12:24.43571Z","shell.execute_reply.started":"2022-04-14T07:12:19.746171Z","shell.execute_reply":"2022-04-14T07:12:24.434985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_users = len(new_dataset.userId.unique())\nn_products =  len(new_dataset.ProductId.unique())\nn_latent_factors = 50\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:12:24.437057Z","iopub.execute_input":"2022-04-14T07:12:24.437327Z","iopub.status.idle":"2022-04-14T07:12:24.473643Z","shell.execute_reply.started":"2022-04-14T07:12:24.437291Z","shell.execute_reply":"2022-04-14T07:12:24.472918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(data, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:12:24.474812Z","iopub.execute_input":"2022-04-14T07:12:24.475648Z","iopub.status.idle":"2022-04-14T07:12:25.138486Z","shell.execute_reply.started":"2022-04-14T07:12:24.47561Z","shell.execute_reply":"2022-04-14T07:12:25.137775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_count_recommendation_list(pred_user):\n  counts = collections.Counter(c for l in pred_user for c in l)\n  return dict(counts)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:12:25.139864Z","iopub.execute_input":"2022-04-14T07:12:25.140142Z","iopub.status.idle":"2022-04-14T07:12:25.144846Z","shell.execute_reply.started":"2022-04-14T07:12:25.140108Z","shell.execute_reply":"2022-04-14T07:12:25.144088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\nfrom typing import List\nimport scipy.sparse as sp\nfrom scipy.stats import entropy\nfrom functools import reduce\nfrom math import log, e\n\ndef get_customer_satisfaction(pred_u,k):\n  edt = {}\n  rating_list = defaultdict(list)\n  pred = pred_u.copy().groupby(['userId'])\n  for userId in pred.groups.keys():\n    sorted_pred_group = pred.get_group(userId).sort_values(['predicted_rating'], ascending = False)\n    top_k = sorted_pred_group[:k]\n    top_k_g = top_k.groupby(by='userId')\n    for userId in top_k_g.groups.keys():\n      top_k_user_list = top_k_g.get_group(userId)\n      for _, groups in top_k_user_list.iterrows():\n        diff_ratings = groups['predicted_rating'] - groups['ratings']\n        rating_list.setdefault(groups['userId'], []).append(diff_ratings)\n      edt[userId] = (np.sum(rating_list.get(userId)))\n  #def predict_reco_list(row):\n  #  if row['userId'] in customer_satisfaction.keys():\n  #    return customer_satisfaction.get(row['userId'])\n\n  #predicted['customer_satisfaction'] = predicted.apply(predict_reco_list, axis=1)\n  #print(\"Average satisfaction is {}\".format(np.sum(predicted['customer_satisfaction'])))\n  return np.mean(list(edt.values()))\n\ndef personalization(predicted: List[list]) -> float:\n    \"\"\"\n    Personalization measures recommendation similarity across users.\n    A high score indicates good personalization (user's lists of recommendations are different).\n    A low score indicates poor personalization (user's lists of recommendations are very similar).\n    A model is \"personalizing\" well if the set of recommendations for each user is different.\n    Parameters:\n    ----------\n    predicted : a list of lists\n        Ordered predictions\n        example: [['X', 'Y', 'Z'], ['X', 'Y', 'Z']]\n    Returns:\n    -------\n        The personalization score for all recommendations.\n    \"\"\"\n\n    def make_rec_matrix(predicted: List[list]) -> sp.csr_matrix:\n        df = pd.DataFrame(data=predicted).reset_index().melt(\n            id_vars='index', value_name='item',\n        )\n        chunk_size = 5000\n        chunks = [x for x in range(0, df.shape[0], chunk_size)]\n        df = pd.concat([df.iloc[ chunks[i]:chunks[i + 1] - 1 ].pivot(index='index', columns='item', values='item') for i in range(0, len(chunks) - 1)])\n\n        #df = df[['index', 'item']].pivot(index='index', columns='item', values='item')\n        df = pd.notna(df)*1\n        rec_matrix = sp.csr_matrix(df.values)\n        return rec_matrix\n\n    #create matrix for recommendations\n    predicted = np.array(predicted)\n    rec_matrix_sparse = make_rec_matrix(predicted)\n\n    #calculate similarity for every user's recommendation list\n    similarity = cosine_similarity(X=rec_matrix_sparse, dense_output=False)\n    return similarity\n  \ndef get_metrics(productList, predictions):\n  \n  # To calculate the prediction coverage\n  cf_coverage = recmetrics.prediction_coverage(predictions, productList)\n  print(\"The coverage score is {} \\n\".format(cf_coverage))\n\n  # To calculate the novelty\n  nov = new_dataset.ProductId.value_counts()\n  pop = dict(nov)\n  users = new_dataset['userId'].value_counts()\n  cf_novelty,cf_mselfinfo_list = recmetrics.novelty(predictions, pop, len(users), 10)\n  print(\"The novelty score is {} \\n\".format(cf_novelty))\n\n  # To Calculate the average diversity\n  #diversity = recmetrics.personalization(predictions.tolist())\n  #print(\"Average diversity {} \\n\".format(diversity))\n\n  # To calculate the diversity for each user and then take the average\n  diversity = personalization(predictions.tolist())\n  average = []\n  for c in diversity.toarray():\n    average.append(sum(c)/len(c))\n\n  #print(\"Diversity for each user and average {} \\n\".format(1 - (sum(average)/diversity.shape[0])))\n\ndef get_f1_score(predictions, k):\n    threshold = 4\n    # First map the predictions to each user.\n    user_est_rating = defaultdict(list)\n    for uid, iid, r_ui, est, _ in predictions:\n        user_est_rating[uid].append((est, r_ui))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    f1_score = dict()\n    for uid, user_ratings in user_est_rating.items():\n        user_ratings.sort(key=lambda x:x[0], reverse=True)\n        # Number of relevant items\n        n_rel = sum((r_ui >= threshold) for (_, r_ui) in user_ratings)\n        if math.isnan(n_rel):\n          print(\"nan value for rel\") \n        # Number of recommended items in top k\n        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n        if math.isnan(n_rec_k):\n          print(\"nan value for rel\") \n        # Number of relevant and recommended items in top k\n        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n                              for (est, true_r) in user_ratings[:k])\n        if math.isnan(n_rel_and_rec_k):\n          print(\"nan value for rel and rec\") \n        precision = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n        recall = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n        f1_score[uid] = 2 * ((precision * recall)/(precision+recall)) if (precision + recall) != 0 else 0\n    return f1_score\n\ndef get_f1_score_nn(predictions, k):\n    threshold = 4\n    # First map the predictions to each user.\n    user_est_rating = defaultdict(list)\n    #for uid, iid, r_ui, est in predictions:\n    #    user_est_rating[uid].append((est, r_ui))\n    for index, row in predictions.iterrows():\n        user_est_rating[row['userId']].append((row['predicted_rating'], row['ratings']))\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    f1_score = dict()\n    for uid, user_ratings in user_est_rating.items():\n        user_ratings.sort(key=lambda x:x[0], reverse=True)\n        # Number of relevant items\n        n_rel = sum((r_ui >= threshold) for (_, r_ui) in user_ratings)\n        if math.isnan(n_rel):\n          print(\"nan value for rel\") \n        # Number of recommended items in top k\n        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n        if math.isnan(n_rec_k):\n          print(\"nan value for rel\") \n        # Number of relevant and recommended items in top k\n        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n                              for (est, true_r) in user_ratings[:k])\n        if math.isnan(n_rel_and_rec_k):\n          print(\"nan value for rel and rec\") \n        precision = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n        recall = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n        f1_score[uid] = 2 * ((precision * recall)/(precision+recall)) if (precision + recall) != 0 else 0\n    return f1_score\n  \ndef get_shannon_entropy(predictions, product_list, l_recommendation_items, no_of_recommendations):\n  counts = get_count_recommendation_list(predictions['cf_predictions'])\n  entropy_users = {}\n  pi = list()\n  column_name = 'entropy_' + str(no_of_recommendations)\n  for userId, row in predictions.iterrows():\n    for product in row['cf_predictions']:\n      pi.append(counts.get(product)/l_recommendation_items)\n    predictions[column_name] = entropy(pi)\n    pi.clear()\n  print(\"Average diversity using shannon entropy for {} no of recommendations is {} \\n\".format(no_of_recommendations, np.mean(predictions[column_name])))\n\ndef get_shannon_entropy_d(predictions, product_list, l_recommendation_items, no_of_recommendations):\n  #counts = get_count_recommendation_list(predictions['cf_predictions'])\n  recommendation_items = list(np.concatenate(predictions['cf_predictions']).flat)\n  print(\"length of recommendation items\" , len(recommendation_items))\n  #count_recommendation_items = collections.Counter(recommendation_items)\n  product,counts = np.unique(recommendation_items, return_counts=True)\n  norm_counts = counts / counts.sum()\n  #print(\"total_count\", np.sum(list(count_recommendation_items.values())))\n  pi = list()\n  #for product in product_list:\n  #  if product in count_recommendation_items.keys():\n  #    pi.append(count_recommendation_items.get(product)/len(recommendation_items))\n  entropy_users = entropy(norm_counts)\n  #print(\"Average diversity using shannon entropy for {} no of recommendations is {} \\n\".format(-np.sum(pi*np.log2(pi)), no_of_recommendations))\n  print(\"Average diversity using shannon entropy for {} no of recommendations is {} \\n\".format(entropy_users, no_of_recommendations))\n\ndef get_shannon_entropy_new(predictions, product_list, l_recommendation_items, no_of_recommendations,product_rating):\n  counts = get_count_recommendation_list(predictions['cf_predictions'])\n  #count_recommendation_items = collections.Counter(recommendation_items)\n  ent = 0\n  for product in product_list:\n    if product in counts.keys():\n      pi = (counts.get(product)/l_recommendation_items) / product_rating.get(product)\n      ent -= pi * log(pi,2)\n      \n  #entropy_users = entropy(norm_counts)\n  #print(\"Average diversity using shannon entropy for {} no of recommendations is {} \\n\".format(-np.sum(pi*np.log2(pi)), no_of_recommendations))\n  print(\"Average diversity using shannon entropy for {} no of recommendations is {} \\n\".format(ent, no_of_recommendations))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:50:53.590069Z","iopub.execute_input":"2022-04-14T10:50:53.590366Z","iopub.status.idle":"2022-04-14T10:50:53.63079Z","shell.execute_reply.started":"2022-04-14T10:50:53.590334Z","shell.execute_reply":"2022-04-14T10:50:53.629903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_users_predictions(userId, n, model):\n    recommended_items = pd.DataFrame(model.loc[userId])\n    recommended_items.columns = [\"predicted_rating\"]\n    recommended_items = recommended_items.sort_values('predicted_rating', ascending=False)    \n    recommended_items = recommended_items.head(n)\n    return recommended_items.index.tolist()\n\n\ndef get_top_n(pred, n=10):\n    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n\n    Args:\n        predictions(list of Prediction objects): The list of predictions, as\n            returned by the test method of an algorithm.\n        n(int): The number of recommendation to output for each user. Default\n            is 10.\n\n    Returns:\n    A dict where keys are user (raw) ids and values are lists of tuples:\n        [(raw item id, rating estimation), ...] of size n.\n    \"\"\"\n\n    # First map the predictions to each user.\n    predictions = pred.copy()\n    top_n = defaultdict(list)\n    for idx, userId, productId, true_r, est in predictions.itertuples():\n        top_n[userId].append((productId, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for userId, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[userId] = user_ratings[:n]\n\n    ratings = {}\n    for uid, user_ratings in top_n.items():\n      iid_list = []\n      for iid, _ in user_ratings:\n        iid_list.append(iid)\n      ratings[uid] = iid_list\n    \n    def predict_reco_list(row):\n      if row['userId'] in ratings.keys():\n        return ratings.get(row['userId'])\n\n    predictions['cf_predictions'] = predictions.apply(predict_reco_list, axis=1)\n    return predictions\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:12:25.181283Z","iopub.execute_input":"2022-04-14T07:12:25.181638Z","iopub.status.idle":"2022-04-14T07:12:25.193958Z","shell.execute_reply.started":"2022-04-14T07:12:25.181602Z","shell.execute_reply":"2022-04-14T07:12:25.193242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cal_f1(test_pred, no_of_recommendations):\n  results=[] \n  for i in no_of_recommendations:  \n     f1_score = get_f1_score(test_pred, k=i)\n     for uid,score in f1_score.items():\n       if math.isnan(score):\n         print(\"nan value for uid {} value {}\".format(uid, score) ) \n     average_f1_score = sum(score for score in f1_score.values())/ len(f1_score)\n     results.append(average_f1_score)\n  return results\ndef cal_f1_nn(test_pred, no_of_recommendations):\n  results=[] \n  for i in no_of_recommendations:  \n     f1_score = get_f1_score_nn(test_pred, k=i)\n     for uid,score in f1_score.items():\n       if math.isnan(score):\n         print(\"nan value for uid {} value {}\".format(uid, score) ) \n     average_f1_score = sum(score for score in f1_score.values())/ len(f1_score)\n     results.append(average_f1_score)\n  return results","metadata":{"execution":{"iopub.status.busy":"2022-04-14T09:56:46.130877Z","iopub.execute_input":"2022-04-14T09:56:46.131139Z","iopub.status.idle":"2022-04-14T09:56:46.139081Z","shell.execute_reply.started":"2022-04-14T09:56:46.131109Z","shell.execute_reply":"2022-04-14T09:56:46.138332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install scikit-surprise\n# !conda install -y -c conda-forge scikit-surprise # If you use conda on a non-Colab environment","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:12:25.205515Z","iopub.execute_input":"2022-04-14T07:12:25.206088Z","iopub.status.idle":"2022-04-14T07:12:35.256504Z","shell.execute_reply.started":"2022-04-14T07:12:25.206036Z","shell.execute_reply":"2022-04-14T07:12:35.255679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from surprise import SVD\nfrom surprise import KNNWithMeans\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\nfrom surprise import Reader\nfrom surprise.model_selection import train_test_split\nfrom surprise import accuracy\nno_of_factors = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nno_of_recommendations = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\nreader = Reader()\nratings_dataset = Dataset.load_from_df(new_dataset[['userId', 'ProductId', 'Ratings']],reader)\ntrainset, testset = train_test_split(ratings_dataset, test_size=0.2,random_state=100)\nmae_svd = list()\nfor i in no_of_factors:\n  algo = SVD(n_factors=i)\n  algo.fit(trainset)\n  test_pred = algo.test(testset)\n  mae_svd.append(accuracy.mae(test_pred))\n  print(\"Mean Absolute Error for value k {} is \".format(i), accuracy.mae(test_pred))\nsvd_f1_score = cal_f1(test_pred, no_of_recommendations)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:12:35.260084Z","iopub.execute_input":"2022-04-14T07:12:35.260326Z","iopub.status.idle":"2022-04-14T07:13:32.998265Z","shell.execute_reply.started":"2022-04-14T07:12:35.260298Z","shell.execute_reply":"2022-04-14T07:13:32.997312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from surprise import SVD\nfrom surprise import KNNWithMeans\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\nfrom surprise import Reader\nfrom surprise.model_selection import train_test_split\nk = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nno_of_recommendations = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\nmae_knn = list()\nfor i in k:\n  algo = KNNWithMeans(k=i, sim_options={'name':'pearson','user_based': True})\n  algo.fit(trainset)\n  test_pred = algo.test(testset)\n  mae_knn.append(accuracy.mae(test_pred))\n  print(\"Mean Absolute Error for value k {} is \".format(i), accuracy.mae(test_pred))\nknn_f1_score = cal_f1(test_pred, no_of_recommendations)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:18:39.714548Z","iopub.execute_input":"2022-04-14T10:18:39.714852Z","iopub.status.idle":"2022-04-14T10:19:01.223067Z","shell.execute_reply.started":"2022-04-14T10:18:39.714819Z","shell.execute_reply":"2022-04-14T10:19:01.221833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import sparse\nfrom tqdm import tqdm\n\ndef get_predictions(predictions, no_of_recommendations):\n  pred_user = predictions.copy().groupby('userId', as_index=False)['productId'].agg({'ratings': (lambda x: list(set(x)))})\n  pred_user = pred_user.set_index(\"userId\")\n  chunk_size = 5000\n  chunks = [x for x in range(0, predictions.shape[0], chunk_size)]\n  #model = pd.DataFrame()\n\n  for i in tqdm(range(0, len(chunks) - 1)):\n    chunk_df = predictions.iloc[ chunks[i]:chunks[i + 1] - 1]\n    interactions = (chunk_df.groupby(['userId', 'productId'])['ratings']\n      .sum()\n      .unstack()\n      .reset_index()\n      .fillna(0)\n      .set_index('userId')\n    )\n    model = model.append(interactions, sort=False) \n    \n  #model = predictions.concat([predictions.iloc[ chunks[i]:chunks[i + 1] - 1 ].pivot(index='userId', columns='productId', values='ratings') for i in range(0, len(chunks) - 1)])\n  chunk_size = 5000\n  chunks = [x for x in range(0, df.shape[0], chunk_size)]\n  model = predictions.concat([predictions.iloc[ chunks[i]:chunks[i + 1] - 1 ].pivot(index='userId', columns='productId', values='ratings') for i in range(0, len(chunks) - 1)])\n  #model = predictions.pivot_table(index='userId', columns='productId', values='ratings').fillna(0)\n  cf_recs = [] = []\n  for user in pred_user.index:\n    cf_predictions = get_users_predictions(user, no_of_recommendations, model)\n    cf_recs.append(cf_predictions)\n  pred_user['cf_predictions'] = cf_recs\n  return pred_user","metadata":{"execution":{"iopub.status.busy":"2022-04-14T11:08:17.748174Z","iopub.execute_input":"2022-04-14T11:08:17.748888Z","iopub.status.idle":"2022-04-14T11:08:17.759071Z","shell.execute_reply.started":"2022-04-14T11:08:17.74885Z","shell.execute_reply":"2022-04-14T11:08:17.758276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom keras.preprocessing.text import one_hot\nuser_encoder = LabelEncoder()\nproduct_encoder = LabelEncoder()\n\ntrain_user_ids = np.array([one_hot(d,10) for d in train['userId']])\ntrain_product_ids = np.array([one_hot(d,10) for d in train['ProductId']])\ntest_product_ids = np.array([one_hot(d,10) for d in test['ProductId']])\ntest_user_ids = np.array([one_hot(d,10) for d in test['userId']])\n#train_user_ids = user_encoder.fit_transform(train.userId)\n#train_product_ids = product_encoder.fit_transform(train.ProductId)\n#test_user_ids = user_encoder.fit_transform(test.userId)\n#test_product_ids = product_encoder.fit_transform(test.ProductId)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:13:55.758082Z","iopub.execute_input":"2022-04-14T07:13:55.758523Z","iopub.status.idle":"2022-04-14T07:14:33.520513Z","shell.execute_reply.started":"2022-04-14T07:13:55.758486Z","shell.execute_reply":"2022-04-14T07:14:33.519765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_users= train_user_ids.max()+1\nnum_products = train_product_ids.max() + 1","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:14:33.521825Z","iopub.execute_input":"2022-04-14T07:14:33.522055Z","iopub.status.idle":"2022-04-14T07:14:33.530298Z","shell.execute_reply.started":"2022-04-14T07:14:33.522024Z","shell.execute_reply":"2022-04-14T07:14:33.529549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers.normalization.batch_normalization import BatchNormalization\nimport tensorflow as tf\nfrom keras.layers import Input, Embedding, Flatten, Dense, Concatenate, dot\n\ndef get_ncf_model(no_of_factors):\n\n  product_input = Input(shape = [1], name = \"Product-Input\")\n  user_input = Input(shape = [1], name = \"User-Input\")\n\n\n\n  # Product embedding for GMF\n  gmf_product_embedding = Embedding(n_products, no_of_factors, name= \"GMF-Product-Embedding\")(product_input)\n  \n\n  # User embedding for GMF\n  gmf_user_embedding = Embedding(n_users, no_of_factors, name = \"GMF-User-Embedding\")(user_input)\n  \n  # GMF layers\n  gmf_product_vec = Flatten(name = \"GMF-Flatten-Products\")(gmf_product_embedding)\n  gmf_user_vec = Flatten(name = \"GMF-Flatten-Users\")(gmf_user_embedding)\n  gmf_output = Multiply()([gmf_user_vec, gmf_product_vec])\n\n  \n  # Product embedding for MLP\n  mlp_product_embedding = Embedding(num_products, no_of_factors, name= \"MLP-Product-Embedding\")(product_input)\n  \n\n  # User embedding for MLP\n  mlp_user_embedding = Embedding(num_users, no_of_factors, name = \"MLP-User-Embedding\")(user_input)\n\n  # MLP layers\n\n  mlp_product_vec = Flatten(name = \"MLP-Flatten-Products\")(mlp_product_embedding)\n  mlp_user_vec = Flatten(name = \"MLP-Flatten-Users\")(mlp_user_embedding)\n\n  #Concatenate features\n  conc = Concatenate()([mlp_user_vec, mlp_product_vec])\n\n  fc1 = Dropout(0.2)(conc)\n  fc2 = Dense(64, activation='relu')(fc1)\n  fc3 = BatchNormalization()(fc2)\n  fc4 = Dropout(0.2)(fc3)\n  fc5 = Dense(32, activation='relu')(fc4)\n  fc6 = BatchNormalization()(fc5)\n  fc7 = Dropout(0.2)(fc6)\n\n  fc8 = Dense(16, activation='relu')(fc7)\n  fc9 = Dense(8, activation='relu')(fc8)\n  final_conc = Concatenate()([gmf_output, fc9])\n  output = Dense(1, activation='relu')(final_conc)\n\n  #Create model and compile it\n  model = Model([user_input, product_input], output)\n  model.compile('adam', 'mean_absolute_error')\n  return model","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:14:33.531928Z","iopub.execute_input":"2022-04-14T07:14:33.532238Z","iopub.status.idle":"2022-04-14T07:14:33.54615Z","shell.execute_reply.started":"2022-04-14T07:14:33.532201Z","shell.execute_reply":"2022-04-14T07:14:33.545242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(nn_f1_score)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:20:02.944084Z","iopub.execute_input":"2022-04-14T10:20:02.944368Z","iopub.status.idle":"2022-04-14T10:20:02.94959Z","shell.execute_reply.started":"2022-04-14T10:20:02.944335Z","shell.execute_reply":"2022-04-14T10:20:02.948147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_of_factors = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nmae_ncf = list()\nfor k in no_of_factors:\n  model = get_ncf_model(k)\n  model.fit([train_user_ids, train_product_ids], train['Ratings'], epochs=3)\n  predicton = model.predict([test_user_ids, test_product_ids])\n  mae_ncf.append(mean_absolute_error(test['Ratings'], predicton))\n  print(\"Mean Absolute Error for value k {} is \".format(k), mean_absolute_error(test['Ratings'], predicton))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T07:14:33.547721Z","iopub.execute_input":"2022-04-14T07:14:33.547979Z","iopub.status.idle":"2022-04-14T09:46:25.959281Z","shell.execute_reply.started":"2022-04-14T07:14:33.547946Z","shell.execute_reply":"2022-04-14T09:46:25.958535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_of_recommendations = [5, 10, 20, 30, 40, 50 , 60, 70, 89, 90, 100]\nprediction = model.predict([test_user_ids, test_product_ids])\npredicted_df = pd.DataFrame({'userId': test['userId'], 'productId': test['ProductId'], 'ratings': test['Ratings']})\npredicted_df['predicted_rating'] = prediction\n\nnn_f1_score = cal_f1_nn(predicted_df, no_of_recommendations)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:23:17.912183Z","iopub.execute_input":"2022-04-14T10:23:17.912648Z","iopub.status.idle":"2022-04-14T10:27:52.262069Z","shell.execute_reply.started":"2022-04-14T10:23:17.912608Z","shell.execute_reply":"2022-04-14T10:27:52.261315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nk = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\nplt.xlabel('No of Recommendations')\nplt.ylabel('Mean Absolute Error')\nax=plt.gca()\nax.locator_params('y', nbins=15)\nplt.locator_params('x', nbins=20)\nplt.plot(k, mae_knn, label = \"KNN\")\nplt.plot(k, mae_svd, label = \"SVD\")\nplt.plot(k, mae_ncf, label = \"DNN\")\n\nplt.scatter(k,mae_knn,s=50,color='red',zorder=2)\nplt.scatter(k,mae_svd,s=50,color='green',zorder=2)\nplt.scatter(k,mae_ncf,s=50,color='blue',zorder=2)\n\nplt.legend()\nplt.show()\nplt.xlabel('No of Recommendations')\nplt.ylabel('F1 Score')\nax=plt.gca()\nax.locator_params('y', nbins=10)\nplt.locator_params('x', nbins=20)\nplt.scatter(no_of_recommendations,knn_f1_score,s=50,color='red',zorder=2)\nplt.scatter(no_of_recommendations,svd_f1_score,s=50,color='green',zorder=2)\nplt.scatter(no_of_recommendations,nn_f1_score,s=50,color='brown',zorder=2)\nplt.plot(no_of_recommendations, knn_f1_score, label = \"KNN\")\nplt.plot(no_of_recommendations, svd_f1_score, label = \"SVD\")\nplt.plot(no_of_recommendations, nn_f1_score, label = \"DNN\")\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T10:29:23.179832Z","iopub.execute_input":"2022-04-14T10:29:23.180091Z","iopub.status.idle":"2022-04-14T10:29:23.622762Z","shell.execute_reply.started":"2022-04-14T10:29:23.180062Z","shell.execute_reply":"2022-04-14T10:29:23.622012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import functools\n\nno_of_recommendations = [5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\nfor k in no_of_recommendations:\n  pred_user = get_predictions(predicted_df, k)\n  l_recommendation_items = functools.reduce(lambda count, l: count + len(l), pred_user['cf_predictions'], 0)\n  print(\"The metrics for the number of recommendations {} \\n\".format(k))\n  get_shannon_entropy_new(pred_user, product_list, l_recommendation_items, k, product_rating)\n  get_metrics(product_list, pred_user['cf_predictions'])","metadata":{"execution":{"iopub.status.busy":"2022-04-14T11:12:34.466905Z","iopub.execute_input":"2022-04-14T11:12:34.467648Z","iopub.status.idle":"2022-04-14T11:12:34.557602Z","shell.execute_reply.started":"2022-04-14T11:12:34.467534Z","shell.execute_reply":"2022-04-14T11:12:34.556604Z"},"trusted":true},"execution_count":null,"outputs":[]}]}